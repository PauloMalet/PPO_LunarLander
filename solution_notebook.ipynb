{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/School/PPO_LunarLander/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_std):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.Tanh(), nn.Linear(64, 32), nn.Tanh(), nn.Linear(32, action_dim), nn.Tanh()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.Tanh(), nn.Linear(64, 32), nn.Tanh(), nn.Linear(32, 1)\n",
    "        )\n",
    "        self.action_var = torch.full((action_dim,), action_std * action_std).to(device)\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state, memory):\n",
    "        action_mean = self.actor(state)\n",
    "        cov_mat = torch.diag(self.action_var).to(device)\n",
    "\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.detach()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_mean = self.actor(state)\n",
    "        action_var = self.action_var.expand_as(action_mean)\n",
    "        cov_mat = torch.diag_embed(action_var).to(device)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.critic(state)\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        self.action_var = torch.full((self.action_var.shape[0],), new_action_std * new_action_std).to(device)\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, action_std).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, action_std).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        self.policy.set_action_std(new_action_std)\n",
    "        self.policy_old.set_action_std(new_action_std)\n",
    "\n",
    "    def select_action(self, state, memory):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\n",
    "\n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate of rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # Convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(memory.states).to(device), 1).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(memory.actions).to(device), 1).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs), 1).to(device).detach()\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Finding ratios (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "\n",
    "            # Take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Test:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path=\"trained_model.pth\",\n",
    "        env_name=\"LunarLander-v3\",\n",
    "        max_timesteps=350,\n",
    "        action_std=0.01,\n",
    "        K_epochs=40,\n",
    "        eps_clip=0.2,\n",
    "        gamma=0.99,\n",
    "        lr=0.0003,\n",
    "        betas=(0.9, 0.999),\n",
    "        n_simulations=100,\n",
    "        env_kwargs=None,\n",
    "    ):\n",
    "        self.model_path = model_path\n",
    "        self.n_simulations = n_simulations\n",
    "        self.env_name = env_name\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.action_std = action_std\n",
    "        self.K_epochs = K_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.env_kwargs = env_kwargs or {\"continuous\": True}\n",
    "\n",
    "    def run_simulation(self, env, ppo, memory):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        timesteps = 0\n",
    "\n",
    "        for t in range(self.max_timesteps):\n",
    "            action = ppo.select_action(state, memory)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            timesteps += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return total_reward, timesteps\n",
    "\n",
    "    def plot_results(self, rewards, timesteps):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        ax1.hist(rewards, bins=20, edgecolor=\"black\")\n",
    "        ax1.set_title(\"Distribution of Rewards\")\n",
    "        ax1.set_xlabel(\"Reward\")\n",
    "        ax1.set_ylabel(\"Frequency\")\n",
    "        ax1.set_xlim(-50, max(rewards))\n",
    "        ax1.axvline(\n",
    "            np.mean(rewards), color=\"r\", linestyle=\"dashed\", linewidth=1, label=f\"Mean: {np.mean(rewards):.1f}\"\n",
    "        )\n",
    "        ax1.legend()\n",
    "\n",
    "        ax2.hist(timesteps, bins=20, edgecolor=\"black\")\n",
    "        ax2.set_title(\"Distribution of Episode Lengths\")\n",
    "        ax2.set_xlabel(\"Timesteps\")\n",
    "        ax2.set_ylabel(\"Frequency\")\n",
    "        ax2.axvline(\n",
    "            np.mean(timesteps), color=\"r\", linestyle=\"dashed\", linewidth=1, label=f\"Mean: {np.mean(timesteps):.1f}\"\n",
    "        )\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"test_results.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def load_and_test_model(self):\n",
    "        env_no_render = gym.make(self.env_name, **self.env_kwargs)\n",
    "        render_kwargs = {**self.env_kwargs, \"render_mode\": \"human\"}\n",
    "        env_render = gym.make(self.env_name, **render_kwargs)\n",
    "\n",
    "        state_dim = env_no_render.observation_space.shape[0]\n",
    "        action_dim = env_no_render.action_space.shape[0]\n",
    "\n",
    "        ppo = PPO(\n",
    "            state_dim, action_dim, self.action_std, self.lr, self.betas, self.gamma, self.K_epochs, self.eps_clip\n",
    "        )\n",
    "        state_dict = torch.load(self.model_path, weights_only=True, map_location=torch.device(\"cpu\"))\n",
    "        ppo.policy_old.load_state_dict(state_dict)\n",
    "        ppo.policy_old.eval()\n",
    "\n",
    "        memory = Memory()\n",
    "        rewards = []\n",
    "        timesteps = []\n",
    "        print(f\"Running {self.n_simulations} simulations...\")\n",
    "\n",
    "        for i in range(self.n_simulations):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Simulation {i}/{self.n_simulations}\")\n",
    "\n",
    "            if i < self.n_simulations - 1:\n",
    "                reward, t = self.run_simulation(env_no_render, ppo, memory)\n",
    "            else:\n",
    "                reward, t = self.run_simulation(env_render, ppo, memory)\n",
    "\n",
    "            rewards.append(reward)\n",
    "            timesteps.append(t)\n",
    "            memory.clear_memory()\n",
    "\n",
    "        env_no_render.close()\n",
    "        env_render.close()\n",
    "\n",
    "        avg_reward = np.mean(rewards)\n",
    "        std_reward = np.std(rewards)\n",
    "        avg_timesteps = np.mean(timesteps)\n",
    "        std_timesteps = np.std(timesteps)\n",
    "\n",
    "        # Plot results\n",
    "        self.plot_results(rewards, timesteps)\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"\\nTest Results Summary:\")\n",
    "        print(f\"Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "        print(f\"Average Episode Length: {avg_timesteps:.2f} ± {std_timesteps:.2f}\")\n",
    "        print(\"Results visualization saved as 'test_results.png'\")\n",
    "\n",
    "        return rewards, timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 simulations...\n",
      "Simulation 0/100\n",
      "Simulation 10/100\n",
      "Simulation 20/100\n",
      "Simulation 30/100\n",
      "Simulation 40/100\n",
      "Simulation 50/100\n",
      "Simulation 60/100\n",
      "Simulation 70/100\n",
      "Simulation 80/100\n",
      "Simulation 90/100\n",
      "\n",
      "Test Results Summary:\n",
      "Average Reward: 224.69 ± 89.67\n",
      "Average Episode Length: 228.55 ± 43.67\n",
      "Results visualization saved as 'test_results.png'\n"
     ]
    }
   ],
   "source": [
    "# Test model already trained\n",
    "\n",
    "test = Test()\n",
    "_, _ = test.load_and_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "\n",
    "class Train:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_name=\"LunarLander-v3\",\n",
    "        render=True,\n",
    "        render_interval=500,\n",
    "        log_interval=20,\n",
    "        max_episodes=1500,\n",
    "        max_timesteps=350,\n",
    "        update_timestep=2000,\n",
    "        action_std=0.5,\n",
    "        K_epochs=40,\n",
    "        eps_clip=0.2,\n",
    "        gamma=0.99,\n",
    "        lr=0.0003,\n",
    "        betas=(0.9, 0.999),\n",
    "        random_seed=None,\n",
    "        decay_threshold=100,\n",
    "        decay_speed=0.93,\n",
    "        env_kwargs=None,\n",
    "    ):\n",
    "        self.env_name = env_name\n",
    "        self.render = render\n",
    "        self.render_interval = render_interval\n",
    "        self.log_interval = log_interval\n",
    "        self.time_step = 0\n",
    "        self.env_kwargs = env_kwargs or {\"continuous\": True}\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.update_timestep = update_timestep\n",
    "        self.action_std = action_std\n",
    "        self.K_epochs = K_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.decay_threshold = decay_threshold\n",
    "        self.decay_speed = decay_speed\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Environment setup\n",
    "        self.env = gym.make(env_name, **self.env_kwargs)\n",
    "        state_dim = self.env.observation_space.shape[0]\n",
    "        action_dim = self.env.action_space.shape[0]\n",
    "\n",
    "        if random_seed:\n",
    "            print(\"Random Seed: {}\".format(random_seed))\n",
    "            torch.manual_seed(random_seed)\n",
    "            self.env.seed(random_seed)\n",
    "\n",
    "        self.memory = Memory()\n",
    "        from models import PPO\n",
    "\n",
    "        self.ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        curr_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        for t in range(self.max_timesteps):\n",
    "            self.time_step += 1\n",
    "            action = self.ppo.select_action(state, self.memory)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            self.memory.rewards.append(reward)\n",
    "            self.memory.is_terminals.append(done)\n",
    "\n",
    "            if self.time_step % self.update_timestep == 0:\n",
    "                self.ppo.update(self.memory)\n",
    "                self.memory.clear_memory()\n",
    "                self.time_step = 0\n",
    "\n",
    "            curr_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return curr_reward, t\n",
    "\n",
    "    def train(self):\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "        history_avg_reward = []\n",
    "\n",
    "        for i_episode in range(1, self.max_episodes + 1):\n",
    "            if self.render and i_episode % self.render_interval == 0:\n",
    "                render_env = gym.make(self.env_name, render_mode=\"human\", **self.env_kwargs)\n",
    "                r, length = self.play_episode(render_env)\n",
    "                running_reward += r\n",
    "                avg_length += length\n",
    "                print(f\"Render human : length: {length} \\t reward: {r}\")\n",
    "            else:\n",
    "                r, length = self.play_episode(self.env)\n",
    "                running_reward += r\n",
    "                avg_length += length\n",
    "\n",
    "            if i_episode % 500 == 0:\n",
    "                torch.save(self.ppo.policy.state_dict(), f\"./PPO_continuous_{self.env_name}.pth\")\n",
    "                print(f\"Saved at episode {i_episode}\")\n",
    "\n",
    "            if i_episode % self.log_interval == 0:\n",
    "                avg_length = int(avg_length / self.log_interval)\n",
    "                running_reward = int((running_reward / self.log_interval))\n",
    "\n",
    "                print(f\"Episode {i_episode} \\t Avg length: {avg_length} \\t Avg reward: {running_reward}\")\n",
    "                history_avg_reward.append((running_reward, avg_length))\n",
    "\n",
    "                if running_reward > self.decay_threshold and self.action_std > 0.1:\n",
    "                    self.action_std = self.action_std * self.decay_speed\n",
    "                    self.ppo.set_action_std(self.action_std)\n",
    "                    print(\"action_std : \", self.action_std)\n",
    "\n",
    "                running_reward = 0\n",
    "                avg_length = 0\n",
    "\n",
    "        with open(\"history_avg_reward.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(str(reward) for reward in history_avg_reward))\n",
    "\n",
    "        return history_avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 \t Avg length: 109 \t Avg reward: -247\n",
      "Episode 40 \t Avg length: 105 \t Avg reward: -304\n",
      "Episode 60 \t Avg length: 101 \t Avg reward: -198\n",
      "Episode 80 \t Avg length: 106 \t Avg reward: -225\n",
      "Episode 100 \t Avg length: 116 \t Avg reward: -173\n",
      "Episode 120 \t Avg length: 105 \t Avg reward: -169\n",
      "Episode 140 \t Avg length: 114 \t Avg reward: -232\n",
      "Episode 160 \t Avg length: 118 \t Avg reward: -142\n",
      "Episode 180 \t Avg length: 113 \t Avg reward: -85\n",
      "Episode 200 \t Avg length: 114 \t Avg reward: -103\n",
      "Episode 220 \t Avg length: 115 \t Avg reward: -129\n",
      "Episode 240 \t Avg length: 114 \t Avg reward: -122\n",
      "Render human : length: 127 \t reward: -88.37317039447605\n",
      "Episode 260 \t Avg length: 113 \t Avg reward: -92\n",
      "Episode 280 \t Avg length: 109 \t Avg reward: -86\n",
      "Episode 300 \t Avg length: 109 \t Avg reward: -111\n",
      "Episode 320 \t Avg length: 101 \t Avg reward: -105\n",
      "Episode 340 \t Avg length: 100 \t Avg reward: -66\n",
      "Episode 360 \t Avg length: 117 \t Avg reward: -56\n",
      "Episode 380 \t Avg length: 109 \t Avg reward: -72\n",
      "Episode 400 \t Avg length: 115 \t Avg reward: -39\n",
      "Episode 420 \t Avg length: 121 \t Avg reward: -15\n",
      "Episode 440 \t Avg length: 124 \t Avg reward: -17\n",
      "Episode 460 \t Avg length: 164 \t Avg reward: 14\n",
      "Episode 480 \t Avg length: 201 \t Avg reward: 37\n",
      "Render human : length: 171 \t reward: -2.2407227153601355\n",
      "Saved at episode 500\n",
      "Episode 500 \t Avg length: 172 \t Avg reward: -4\n",
      "Training completed in 87.48321533203125 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train on 500 episodes, to see that it runs and improves in less than 2 minutes.\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "train = Train(\n",
    "    render=True,\n",
    "    render_interval=250,\n",
    "    log_interval=20,\n",
    "    max_episodes=500,\n",
    "    max_timesteps=300,\n",
    "    update_timestep=2000,\n",
    ")\n",
    "history_avg_reward = train.train()\n",
    "execution_time = time.time() - start_time\n",
    "print(\"Training completed in %s seconds\" % (execution_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
