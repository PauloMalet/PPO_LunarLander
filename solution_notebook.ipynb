{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_std):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.Tanh(), nn.Linear(64, 32), nn.Tanh(), nn.Linear(32, action_dim), nn.Tanh()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64), nn.Tanh(), nn.Linear(64, 32), nn.Tanh(), nn.Linear(32, 1)\n",
    "        )\n",
    "        self.action_var = torch.full((action_dim,), action_std * action_std).to(device)\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state, memory):\n",
    "        action_mean = self.actor(state)\n",
    "        cov_mat = torch.diag(self.action_var).to(device)\n",
    "\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.detach()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_mean = self.actor(state)\n",
    "        action_var = self.action_var.expand_as(action_mean)\n",
    "        cov_mat = torch.diag_embed(action_var).to(device)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.critic(state)\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        self.action_var = torch.full((self.action_var.shape[0],), new_action_std * new_action_std).to(device)\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, action_std).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, action_std).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        self.policy.set_action_std(new_action_std)\n",
    "        self.policy_old.set_action_std(new_action_std)\n",
    "\n",
    "    def select_action(self, state, memory):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\n",
    "\n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate of rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # Convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(memory.states).to(device), 1).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(memory.actions).to(device), 1).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs), 1).to(device).detach()\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Finding ratios (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "\n",
    "            # Take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "\n",
    "class Train:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_name=\"LunarLander-v3\",\n",
    "        render=True,\n",
    "        render_interval=500,\n",
    "        log_interval=20,\n",
    "        max_episodes=1500,\n",
    "        max_timesteps=350,\n",
    "        update_timestep=2000,\n",
    "        action_std=0.5,\n",
    "        K_epochs=40,\n",
    "        eps_clip=0.2,\n",
    "        gamma=0.99,\n",
    "        lr=0.0003,\n",
    "        betas=(0.9, 0.999),\n",
    "        random_seed=None,\n",
    "        decay_threshold=100,\n",
    "        decay_speed=0.93,\n",
    "        env_kwargs=None,\n",
    "    ):\n",
    "        self.env_name = env_name\n",
    "        self.render = render\n",
    "        self.render_interval = render_interval\n",
    "        self.log_interval = log_interval\n",
    "        self.time_step = 0\n",
    "        self.env_kwargs = env_kwargs or {\"continuous\": True}\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.update_timestep = update_timestep\n",
    "        self.action_std = action_std\n",
    "        self.K_epochs = K_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.decay_threshold = decay_threshold\n",
    "        self.decay_speed = decay_speed\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Environment setup\n",
    "        self.env = gym.make(env_name, **self.env_kwargs)\n",
    "        state_dim = self.env.observation_space.shape[0]\n",
    "        action_dim = self.env.action_space.shape[0]\n",
    "\n",
    "        if random_seed:\n",
    "            print(\"Random Seed: {}\".format(random_seed))\n",
    "            torch.manual_seed(random_seed)\n",
    "            self.env.seed(random_seed)\n",
    "\n",
    "        self.memory = Memory()\n",
    "        from models import PPO\n",
    "\n",
    "        self.ppo = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        curr_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        for t in range(self.max_timesteps):\n",
    "            self.time_step += 1\n",
    "            action = self.ppo.select_action(state, self.memory)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            self.memory.rewards.append(reward)\n",
    "            self.memory.is_terminals.append(done)\n",
    "\n",
    "            if self.time_step % self.update_timestep == 0:\n",
    "                self.ppo.update(self.memory)\n",
    "                self.memory.clear_memory()\n",
    "                self.time_step = 0\n",
    "\n",
    "            curr_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return curr_reward, t\n",
    "\n",
    "    def train(self):\n",
    "        running_reward = 0\n",
    "        avg_length = 0\n",
    "        history_avg_reward = []\n",
    "\n",
    "        for i_episode in range(1, self.max_episodes + 1):\n",
    "            if self.render and i_episode % self.render_interval == 0:\n",
    "                render_env = gym.make(self.env_name, render_mode=\"human\", **self.env_kwargs)\n",
    "                r, length = self.play_episode(render_env)\n",
    "                running_reward += r\n",
    "                avg_length += length\n",
    "                print(f\"Render human : length: {length} \\t reward: {r}\")\n",
    "            else:\n",
    "                r, length = self.play_episode(self.env)\n",
    "                running_reward += r\n",
    "                avg_length += length\n",
    "\n",
    "            if i_episode % 500 == 0:\n",
    "                torch.save(self.ppo.policy.state_dict(), f\"./PPO_continuous_{self.env_name}.pth\")\n",
    "                print(f\"Saved at episode {i_episode}\")\n",
    "\n",
    "            if i_episode % self.log_interval == 0:\n",
    "                avg_length = int(avg_length / self.log_interval)\n",
    "                running_reward = int((running_reward / self.log_interval))\n",
    "\n",
    "                print(f\"Episode {i_episode} \\t Avg length: {avg_length} \\t Avg reward: {running_reward}\")\n",
    "                history_avg_reward.append((running_reward, avg_length))\n",
    "\n",
    "                if running_reward > self.decay_threshold and self.action_std > 0.1:\n",
    "                    self.action_std = self.action_std * self.decay_speed\n",
    "                    self.ppo.set_action_std(self.action_std)\n",
    "                    print(\"action_std : \", self.action_std)\n",
    "\n",
    "                running_reward = 0\n",
    "                avg_length = 0\n",
    "\n",
    "        with open(\"history_avg_reward.txt\", \"w\") as file:\n",
    "            file.write(\"\\n\".join(str(reward) for reward in history_avg_reward))\n",
    "\n",
    "        return history_avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 \t Avg length: 97 \t Avg reward: -251\n",
      "Episode 40 \t Avg length: 95 \t Avg reward: -226\n",
      "Episode 60 \t Avg length: 86 \t Avg reward: -153\n",
      "Episode 80 \t Avg length: 104 \t Avg reward: -148\n",
      "Episode 100 \t Avg length: 92 \t Avg reward: -131\n",
      "Episode 120 \t Avg length: 104 \t Avg reward: -127\n",
      "Episode 140 \t Avg length: 97 \t Avg reward: -93\n",
      "Episode 160 \t Avg length: 116 \t Avg reward: -93\n",
      "Episode 180 \t Avg length: 124 \t Avg reward: -70\n",
      "Episode 200 \t Avg length: 125 \t Avg reward: -87\n",
      "Episode 220 \t Avg length: 139 \t Avg reward: -36\n",
      "Episode 240 \t Avg length: 179 \t Avg reward: -50\n",
      "Render human : length: 187 \t reward: -1.6611096619094639\n",
      "Episode 260 \t Avg length: 170 \t Avg reward: -56\n",
      "Episode 280 \t Avg length: 172 \t Avg reward: 7\n",
      "Episode 300 \t Avg length: 213 \t Avg reward: 14\n",
      "Episode 320 \t Avg length: 269 \t Avg reward: 12\n",
      "Episode 340 \t Avg length: 289 \t Avg reward: 90\n",
      "Episode 360 \t Avg length: 292 \t Avg reward: 83\n",
      "Episode 380 \t Avg length: 254 \t Avg reward: 63\n",
      "Episode 400 \t Avg length: 275 \t Avg reward: 95\n",
      "Episode 420 \t Avg length: 258 \t Avg reward: 79\n",
      "Episode 440 \t Avg length: 280 \t Avg reward: 113\n",
      "action_std :  0.465\n",
      "Episode 460 \t Avg length: 277 \t Avg reward: 81\n",
      "Episode 480 \t Avg length: 299 \t Avg reward: 131\n",
      "action_std :  0.43245000000000006\n",
      "Render human : length: 299 \t reward: 179.3058861752503\n",
      "Saved at episode 500\n",
      "Episode 500 \t Avg length: 299 \t Avg reward: 134\n",
      "action_std :  0.40217850000000005\n",
      "Training completed in 95.00231432914734 seconds\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train on 500 episodes, to see that it runs and improves in less than 2 minutes.\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "train = Train(\n",
    "    render=True,\n",
    "    render_interval=250,\n",
    "    log_interval=20,\n",
    "    max_episodes=500,\n",
    "    max_timesteps=300,\n",
    "    update_timestep=2000,\n",
    ")\n",
    "history_avg_reward = train.train()\n",
    "execution_time = time.time() - start_time\n",
    "print(\"Training completed in %s seconds\" % (execution_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
